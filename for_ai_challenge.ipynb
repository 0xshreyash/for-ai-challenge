{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "for-ai-challenge.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyashpatodia/for-ai-challenge/blob/master/for_ai_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O5oisfOZu4G",
        "colab_type": "text"
      },
      "source": [
        "# Gotta prune 'em all\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This is Shreyash Patodia's submission to for.ai's pruning challenge.\n",
        "\n",
        "Reproducing the results from this colab should be as simple as running all the cells in order!\n",
        "\n",
        "I've tried to add some context using text and comments to make things easier to understand.\n",
        "\n",
        "## Understanding the challenge\n",
        "\n",
        "* Write a ReLU activated neural network with hidden layer sizes [1000, 1000, 500, 200].\n",
        "\n",
        "* Train network on Fashion-MNIST or MNIST. Choice: Fashion-MNIST.\n",
        "\n",
        "* Prune network using weight pruning and unit pruning. __This is post-hoc pruning and no pruning needs to happen during the training of the network__. Pruning percentages: [0, 25, 50, 60, 70, 80, 90, 95, 97, 99].\n",
        "\n",
        "* Visualize results\n",
        "\n",
        "* Analyze Results\n",
        "\n",
        "* _Bonus_: Speed up neural network execution using new found sparsity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NY7WlfbN0c",
        "colab_type": "code",
        "outputId": "e0d6eb93-6d7f-4ae1-8574-83126e360ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Importing Libraries { form-width: \"200px\", display-mode: \"form\" }\n",
        "from __future__ import print_function\n",
        "from __future__ import division \n",
        "from __future__ import absolute_import \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import copy\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print('Tensorflow version: ', tf.__version__)\n",
        "print(\"Executing eage\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version:  1.14.0\n",
            "Executing eage\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6b6iBwVbDuU",
        "colab_type": "text"
      },
      "source": [
        "## The neural network\n",
        "\n",
        "Defining a keras neural network that can take the input_shape, hidden_sizes and output_size as arguments (choosing keras.Sequential over keras.Model because the model is simple eough to not need any bespoke functionality)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQK3bhiBZuEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network(input_shape, hidden_sizes, output_size):\n",
        "  \"\"\"\n",
        "  Creates a dense network based on the parameters provided.\n",
        "  \n",
        "  Create a network which takes input of shape, input_shape, flattens\n",
        "  the input and then passes it through hidden layers whose sizes are\n",
        "  parameterized by values in hidden_sizes and has an output of size,\n",
        "  output_size.\n",
        "  \n",
        "  Args:\n",
        "    input_shape: Shape of the input. For example, (28, 28).\n",
        "    hidden_sizes: List of the no. of units for hidden layer. For \n",
        "      example, [1000, 1000, 500, 200].\n",
        "    output_size: Number of classes in the output.\n",
        "  \n",
        "  Returns:\n",
        "    model: The model of type keras.Sequential with the given specs.\n",
        "  \"\"\"\n",
        "  \n",
        "  layers = [keras.layers.Flatten(input_shape=input_shape)]\n",
        "  for i in range(len(hidden_sizes)):\n",
        "    layers.append(keras.layers.Dense(hidden_sizes[i],\n",
        "                                     activation=tf.nn.relu,\n",
        "                                     use_bias=False))\n",
        "  layers.append(keras.layers.Dense(output_size, use_bias=False))\n",
        "  model = keras.Sequential(layers)\n",
        "  return model \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4bavvrqyjRi",
        "colab_type": "text"
      },
      "source": [
        "## The Data\n",
        "\n",
        "Choice of dataset: __Fashion MNIST__.\n",
        "\n",
        "We start of by definiing constants based on the data (we also define the constants needed for the neural network here) and then use the function fashion_mnist to get the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2RvOt1_vJ4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Constants { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "input_shape = (28, 28) #@param {type:\"raw\"}\n",
        "hidden_sizes = [1000, 1000, 500, 200] #@param {type:\"raw\"}\n",
        "output_size = 3 #@param {type:\"integer\"}\n",
        "batch_size = 128 #@param {type:\"integer\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk-z2lOAyXZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rescale_data(images):\n",
        "  \"\"\"\n",
        "  Normalizes the images provided to a 0-1 scale.\n",
        "  \n",
        "  Args:\n",
        "    images: inputs to be rescaled.\n",
        "  \n",
        "  \"\"\"\n",
        "  return tf.cast(images / 255.0, tf.float32)\n",
        "\n",
        "def fashion_mnist(batch_size):\n",
        "  \"\"\"\n",
        "  Loads fashion_mnist as tf.data.Dataset.\n",
        "  \n",
        "  Args:\n",
        "    batch_size: Size the dataset should be batched into.\n",
        "    \n",
        "  Returns:\n",
        "    training_data: Batched training_data.\n",
        "    test_data: Batched test_data.\n",
        "  \"\"\"\n",
        "  num_classes = 10\n",
        "  prefetch_size = 10\n",
        "  fashion_mnist = keras.datasets.fashion_mnist\n",
        "  training_data, test_data = fashion_mnist.load_data()\n",
        "  \n",
        "  training_images, training_labels = training_data\n",
        "  test_images, test_labels = test_data\n",
        "  \n",
        "  training_images = rescale_data(training_images)\n",
        "  test_images = rescale_data(test_images)\n",
        "  \n",
        "  training_data = training_images, tf.one_hot(training_labels, num_classes)\n",
        "  test_data = test_images, tf.one_hot(test_labels, num_classes)\n",
        "  \n",
        "  training_data = tf.data.Dataset.from_tensor_slices(training_data)\n",
        "  training_data = training_data.batch(batch_size)\n",
        "  training_data = training_data.shuffle(prefetch_size**2)\n",
        "  training_data = training_data.prefetch(prefetch_size)\n",
        "  \n",
        "  test_data = tf.data.Dataset.from_tensor_slices(test_data)\n",
        "  test_data = test_data.batch(batch_size)\n",
        "  test_data = test_data.shuffle(prefetch_size**2)\n",
        "  test_data = test_data.prefetch(prefetch_size)\n",
        "  \n",
        "  return training_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kTzvDC40-h1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data, test_data = fashion_mnist(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ75OGDV3-YN",
        "colab_type": "text"
      },
      "source": [
        "## Training the network\n",
        "\n",
        "We train the network for 30 epochs using the training data.\n",
        "\n",
        "Optimizer used: Adam.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGWrEoSZ3p0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_network(input_shape, hidden_sizes, output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXWOXrUv4TMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.softmax_cross_entropy,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRbBGUx_4eth",
        "colab_type": "code",
        "outputId": "4d7d9f59-a4cc-4753-f244-61a4ca8529ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(training_data, epochs=30, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.4821 - acc: 0.8234\n",
            "Epoch 2/30\n",
            "469/469 [==============================] - 18s 39ms/step - loss: 0.3601 - acc: 0.8679\n",
            "Epoch 3/30\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.3242 - acc: 0.8802\n",
            "Epoch 4/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.2998 - acc: 0.8887\n",
            "Epoch 5/30\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.2795 - acc: 0.8960\n",
            "Epoch 6/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.2674 - acc: 0.9005\n",
            "Epoch 7/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.2530 - acc: 0.9047\n",
            "Epoch 8/30\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.2398 - acc: 0.9096\n",
            "Epoch 9/30\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.2295 - acc: 0.9125\n",
            "Epoch 10/30\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.2222 - acc: 0.9147\n",
            "Epoch 11/30\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.2149 - acc: 0.9181\n",
            "Epoch 12/30\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.2053 - acc: 0.9217\n",
            "Epoch 13/30\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.1946 - acc: 0.9253\n",
            "Epoch 14/30\n",
            "469/469 [==============================] - 18s 39ms/step - loss: 0.1865 - acc: 0.9277\n",
            "Epoch 15/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1846 - acc: 0.9288\n",
            "Epoch 16/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1738 - acc: 0.9327\n",
            "Epoch 17/30\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.1656 - acc: 0.9352\n",
            "Epoch 18/30\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.1597 - acc: 0.9382\n",
            "Epoch 19/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1493 - acc: 0.9406\n",
            "Epoch 20/30\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.1482 - acc: 0.9416\n",
            "Epoch 21/30\n",
            "469/469 [==============================] - 18s 39ms/step - loss: 0.1413 - acc: 0.9441\n",
            "Epoch 22/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1349 - acc: 0.9467\n",
            "Epoch 23/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1333 - acc: 0.9484\n",
            "Epoch 24/30\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.1260 - acc: 0.9515\n",
            "Epoch 25/30\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.1208 - acc: 0.9520\n",
            "Epoch 26/30\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.1214 - acc: 0.9526\n",
            "Epoch 27/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1139 - acc: 0.9537\n",
            "Epoch 28/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1108 - acc: 0.9566\n",
            "Epoch 29/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1082 - acc: 0.9571\n",
            "Epoch 30/30\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1076 - acc: 0.9589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQxxjbkp7GQq",
        "colab_type": "code",
        "outputId": "da6cc94a-6511-41cb-80b6-d662102998ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# sanity check to make sure the model learnt something.\n",
        "test_loss, test_acc = model.evaluate(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 15ms/step - loss: 0.5297 - acc: 0.8880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP-syqtp6htD",
        "colab_type": "text"
      },
      "source": [
        "## Pruning Networks\n",
        "\n",
        "We define a general prune_model function which takes a parameter called pruning_method which is used to prune the each layer of the network.\n",
        "\n",
        "This allows us to implement layerwise weight and unit pruning and use them interchangably!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1X8_i7ANYe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prune_model(model_weights, pruning_percent, pruning_method):\n",
        "  \"\"\"\n",
        "  Prune model goes through all the hidden layers of a network and prunes them.\n",
        "  \n",
        "  Args:\n",
        "    model_weights: list of model_weights.\n",
        "    pruning_percent: the percentile of weights/units to be pruned.\n",
        "    pruning_method: can be one of `prune_layer_weights` and `prune_layer_units`.\n",
        "  \n",
        "  Returns:\n",
        "    pruned_layerwise_weights: pruned weights of the network.\n",
        "  \"\"\"\n",
        "  layerwise_hidden_weights = model_weights[:-1]\n",
        "  output_weights = model_weights[-1]\n",
        "  pruned_layerwise_weights = [] \n",
        "  for hidden_layer_weights in layerwise_hidden_weights:\n",
        "    pruned_layerwise_weights.append(pruning_method(\n",
        "        hidden_layer_weights, pruning_percent))\n",
        "  pruned_layerwise_weights.append(np.copy(output_weights))\n",
        "  return pruned_layerwise_weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPM1DejN6Mc",
        "colab_type": "text"
      },
      "source": [
        "### Weight Pruning\n",
        "\n",
        "We perform layerwise weight pruning to\n",
        "zero out the bottom k% of the weights of each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F754Smgq6FPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prune_layer_weights(layer_weights, pruning_percent):\n",
        "  \"\"\"\n",
        "  Prunes away `pruning_percent` weights from the network.\n",
        "  \n",
        "  Args:\n",
        "    layer_weights: weights of the layer.\n",
        "    pruning_percent: percent of weights to prune.\n",
        "  \"\"\"\n",
        "  # find abs value \n",
        "  abs_layer_weights = np.absolute(layer_weights)\n",
        "  # find threshold for pruning\n",
        "  threshold = np.percentile(abs_layer_weights, pruning_percent)\n",
        "  \n",
        "  pruned_layer_weights = np.copy(layer_weights)\n",
        "  \n",
        "  # prune (make = 0 ) weights below the threshold\n",
        "  pruned_layer_weights[abs_layer_weights < threshold] = 0\n",
        "  \n",
        "  return pruned_layer_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVFPafNn-vIF",
        "colab_type": "text"
      },
      "source": [
        "### Unit Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vlT2WTU9v-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prune_layer_units(layer_weights, pruning_percent):\n",
        "  \"\"\"\n",
        "  Prunes away `pruning_percent` units from the network.\n",
        "  \n",
        "  Args:\n",
        "    layer_weights: weights of the layer.\n",
        "    pruning_percent: percent of weights to prune.\n",
        "  \"\"\"\n",
        "  # find columwise l2 norm\n",
        "  layerwise_l2_norm = np.linalg.norm(layer_weights, axis=0)\n",
        "  # find threshold based on l2 norm\n",
        "  threshold = np.percentile(layerwise_l2_norm, pruning_percent)\n",
        "   \n",
        "  pruned_layer_weights = np.copy(layer_weights)\n",
        "  \n",
        "  # prune units based on l2 norm (transpose helps use broadcasting of l2_norm\n",
        "  # for pruning).\n",
        "  pruned_layer_weights = np.transpose(pruned_layer_weights) \n",
        "  pruned_layer_weights[layerwise_l2_norm < threshold] = 0\n",
        "  pruned_layer_weights = np.transpose(pruned_layer_weights)\n",
        "  \n",
        "  return pruned_layer_weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_G9c9a9xBW",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating pruning methods\n",
        "\n",
        "\n",
        "Defining a list of all the pruning percentages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7d7y_jd9wct",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "ks = [0, 25, 50, 60, 70, 80, 90, 95, 97, 99] #@param {type:\"raw\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NNmNK-HOY68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model_pruning(model, pruning_method, test_data, ks):\n",
        "  \"\"\"\n",
        "  Evaluates a model pruned using `pruning_method` across different percetiles.\n",
        "  \n",
        "  Args:\n",
        "    model: model whose weights are to be pruned.\n",
        "    pruning_method: pruning strategy. either of `pruned_layer_weights` or\n",
        "      `prune_layer_units`.\n",
        "    test_data: eval data.\n",
        "    ks: list of the percentiles to be pruned away.\n",
        "  \"\"\"\n",
        "  \n",
        "  # model whose weights we'll set to pruned weights\n",
        "  eval_model = create_network(input_shape, hidden_sizes, output_size)\n",
        "  eval_model.compile(\n",
        "      optimizer='adam',\n",
        "      loss=tf.losses.softmax_cross_entropy,\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  model_weights = model.get_weights()\n",
        "  pruning_losses = {}\n",
        "  pruning_accuracies = {}\n",
        "  for k in ks:\n",
        "    pruned_layerwise_weights = prune_model(model_weights, k, pruning_method) \n",
        "    eval_model.set_weights(pruned_layerwise_weights)\n",
        "    loss, acc = eval_model.evaluate(test_data)\n",
        "    pruning_losses[k] = loss\n",
        "    pruning_accuracies[k] = acc\n",
        "  return pruning_losses, pruning_accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fokiFhQs_-Yy",
        "colab_type": "text"
      },
      "source": [
        "### Weight Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkUHomdq-X-k",
        "colab_type": "code",
        "outputId": "8f355ec0-f1e6-4541-e16b-102b99bbb6e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "weight_pruning_losses, weight_pruning_accuracies = eval_model_pruning(\n",
        "  model, prune_layer_weights, test_data, ks=ks)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 15ms/step - loss: 0.5297 - acc: 0.8880\n",
            "79/79 [==============================] - 1s 14ms/step - loss: 0.5208 - acc: 0.8881\n",
            "79/79 [==============================] - 1s 14ms/step - loss: 0.4849 - acc: 0.8880\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.4515 - acc: 0.8879\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.4274 - acc: 0.8840\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.4529 - acc: 0.8656\n",
            "79/79 [==============================] - 1s 14ms/step - loss: 0.7950 - acc: 0.7716\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 1.7662 - acc: 0.3710\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 2.1577 - acc: 0.1938\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 2.2936 - acc: 0.0828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtC72UO1AB7a",
        "colab_type": "text"
      },
      "source": [
        "## Unit Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVxX7iauAF5Q",
        "colab_type": "code",
        "outputId": "e4a5c99c-5873-48d3-ef67-fa3704dbe5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "unit_pruning_losses, unit_pruning_accuracies = eval_model_pruning(\n",
        "  model, prune_layer_units, test_data, ks=ks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5297 - acc: 0.8880\n",
            "79/79 [==============================] - 1s 14ms/step - loss: 0.4874 - acc: 0.8881\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.4037 - acc: 0.8796\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.6054 - acc: 0.8249\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.7663 - acc: 0.7930\n",
            "79/79 [==============================] - 1s 14ms/step - loss: 1.5993 - acc: 0.3373\n",
            "79/79 [==============================] - 1s 14ms/step - loss: 2.1536 - acc: 0.2175\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 2.2673 - acc: 0.1453\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 2.2938 - acc: 0.0954\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 2.3012 - acc: 0.1266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brxqxR_0ATjd",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GorkVxDXwOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_results(accuracies):\n",
        "  \"\"\"\n",
        "  Helps create a table and plot charts for accuracy vs pruning.\n",
        "  \n",
        "  Args:\n",
        "    accuracies: a dict with keys \n",
        "  \"\"\"\n",
        "  \n",
        "  accuracies = pd.DataFrame({\n",
        "      'pruning_percent': list(accuracies.keys()),\n",
        "      'accuracies': list(accuracies.values())\n",
        "  })\n",
        "  \n",
        "  print(accuracies)\n",
        "  \n",
        "  chart = alt.Chart(accuracies, height=300, width=300).mark_line().encode(\n",
        "      x='pruning_percent',\n",
        "      y='accuracies'\n",
        "  ).properties(background='white').interactive()\n",
        "  \n",
        "  chart.display()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-7LiZxLqJvg",
        "colab_type": "text"
      },
      "source": [
        "### Visualize weight pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sld-bEBOAShi",
        "colab_type": "code",
        "outputId": "c7e021f1-181f-4c90-c6e7-a7958ba7c2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "visualize_results(weight_pruning_accuracies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   pruning_percent  accuracies\n",
            "0                0      0.8880\n",
            "1               25      0.8881\n",
            "2               50      0.8880\n",
            "3               60      0.8879\n",
            "4               70      0.8840\n",
            "5               80      0.8656\n",
            "6               90      0.7716\n",
            "7               95      0.3710\n",
            "8               97      0.1938\n",
            "9               99      0.0828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@5\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@3.4.0\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@4\"></script>\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"altair-viz\"></div>\n",
              "  <script>\n",
              "    (function(vegaEmbed) {\n",
              "      var spec = {\"background\": \"white\", \"data\": {\"name\": \"data-fa5f8a2a9610c703b8e48d1c0138a65f\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"pruning_percent\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracies\"}}, \"height\": 300, \"selection\": {\"selector009\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.4.0.json\", \"datasets\": {\"data-fa5f8a2a9610c703b8e48d1c0138a65f\": [{\"pruning_percent\": 0, \"accuracies\": 0.8880000114440918}, {\"pruning_percent\": 25, \"accuracies\": 0.8881000280380249}, {\"pruning_percent\": 50, \"accuracies\": 0.8880000114440918}, {\"pruning_percent\": 60, \"accuracies\": 0.8878999948501587}, {\"pruning_percent\": 70, \"accuracies\": 0.8840000033378601}, {\"pruning_percent\": 80, \"accuracies\": 0.8655999898910522}, {\"pruning_percent\": 90, \"accuracies\": 0.7716000080108643}, {\"pruning_percent\": 95, \"accuracies\": 0.3709999918937683}, {\"pruning_percent\": 97, \"accuracies\": 0.19380000233650208}, {\"pruning_percent\": 99, \"accuracies\": 0.0828000009059906}]}};\n",
              "      var embedOpt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "      function showError(el, error){\n",
              "          el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
              "                          + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                          + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                          + \"See the javascript console for the full traceback.</p>\"\n",
              "                          + '</div>');\n",
              "          throw error;\n",
              "      }\n",
              "      const el = document.getElementById('altair-viz');\n",
              "      vegaEmbed(\"#altair-viz\", spec, embedOpt)\n",
              "        .catch(error => showError(el, error));\n",
              "    })(vegaEmbed);\n",
              "\n",
              "  </script>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_j_TQsTqQqo",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Unit Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMGwcOvIqPyt",
        "colab_type": "code",
        "outputId": "4e8cbc6b-b332-4cf8-e4df-3650c68ccd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "visualize_results(unit_pruning_accuracies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   pruning_percent  accuracies\n",
            "0                0      0.8880\n",
            "1               25      0.8881\n",
            "2               50      0.8796\n",
            "3               60      0.8249\n",
            "4               70      0.7930\n",
            "5               80      0.3373\n",
            "6               90      0.2175\n",
            "7               95      0.1453\n",
            "8               97      0.0954\n",
            "9               99      0.1266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@5\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@3.4.0\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@4\"></script>\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"altair-viz\"></div>\n",
              "  <script>\n",
              "    (function(vegaEmbed) {\n",
              "      var spec = {\"background\": \"white\", \"data\": {\"name\": \"data-c194dea6de8393e7ce57b2601f38e706\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"pruning_percent\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracies\"}}, \"height\": 300, \"selection\": {\"selector010\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.4.0.json\", \"datasets\": {\"data-c194dea6de8393e7ce57b2601f38e706\": [{\"pruning_percent\": 0, \"accuracies\": 0.8880000114440918}, {\"pruning_percent\": 25, \"accuracies\": 0.8881000280380249}, {\"pruning_percent\": 50, \"accuracies\": 0.8795999884605408}, {\"pruning_percent\": 60, \"accuracies\": 0.8248999714851379}, {\"pruning_percent\": 70, \"accuracies\": 0.7929999828338623}, {\"pruning_percent\": 80, \"accuracies\": 0.33730000257492065}, {\"pruning_percent\": 90, \"accuracies\": 0.2175000011920929}, {\"pruning_percent\": 95, \"accuracies\": 0.1453000009059906}, {\"pruning_percent\": 97, \"accuracies\": 0.09539999812841415}, {\"pruning_percent\": 99, \"accuracies\": 0.1265999972820282}]}};\n",
              "      var embedOpt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "      function showError(el, error){\n",
              "          el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
              "                          + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                          + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                          + \"See the javascript console for the full traceback.</p>\"\n",
              "                          + '</div>');\n",
              "          throw error;\n",
              "      }\n",
              "      const el = document.getElementById('altair-viz');\n",
              "      vegaEmbed(\"#altair-viz\", spec, embedOpt)\n",
              "        .catch(error => showError(el, error));\n",
              "    })(vegaEmbed);\n",
              "\n",
              "  </script>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Lk4mwk4PjC",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing Results\n",
        "\n",
        "### General Takeaway\n",
        "\n",
        "The network trained on Fashion-MNIST seems to be fairly robust to post-hoc pruning irrespective of the pruning strategy. We can prove off large fractions of the neural network without seeing a considerable (or proportionate) drop in performance (25% reduction in network size does not decrease accuracy by 25%). \n",
        "\n",
        "The fact that we can prune off such large parts of the network enforces the lottery ticket hypothesis and the fact that some parts of the network are more important than others. It also shows us that the magnitude of the weights/units seems to be a good indicator for their importance to the network which is in line with the intuition that larger weights contribute more to the output and thus, are seemingly more important. \n",
        "\n",
        "I also feel like there might be redundancies in some parts of the network's learning leading it to be robust to weight pruning of upto 70% without showing barely any loss in accuracy. It is only when all the parts learning a specific \"feature\" are lost that performance truly degrades. This is especially true as we go on to prune weights in the network that are not small and are amongst the top 50% of the weights. These weights might have actually had some learning associated with them but some other part of the network might have learnt similar feature but might have done this learning better making the pruned portion less useful and thus, expendable.\n",
        "\n",
        "### Comparing Weight Pruning and Unit Pruning\n",
        "\n",
        "I think of unit pruning as being a special case of weight pruning in the sense that if all the weights in a unit were \"not important\" based on their magnitude to the network then they'd be pruned away by weight pruning. This led to have the intuition that in any case weight pruning should perform at least as well if not better than unit pruning.\n",
        "\n",
        "My inutition was reinforced by the results where at pruning_percent = 70 weight pruning had barely suffered any loss in performance (< 1%) but unit pruning had gone down by almost 10%. This might be because some units might not have a lot of influential weights but by pruning them away we are removing the influential weights that they might have leading to the loss in performance.\n",
        "\n",
        "The downside with weight pruning is that it leads to possibly sparse tensors and optimizing inference time over sprase tensors isn't the easiest. With unit pruning, it is straightforward to remove some neurons and the gains in performance by removing entire columns from our matrices might be signifcantly higher than make our tensors more sparse. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o62KzTUxvXM6",
        "colab_type": "text"
      },
      "source": [
        "## Bonus\n",
        "\n",
        "### Unit Level Sparsity\n",
        "\n",
        "It is fairly straightforward to remove\n",
        "all columns in a matrix which are zero-ed out. But we also want to remove the rows in the next matrix for which the columns in the current one have to zeroed out.\n",
        "\n",
        "Here is an implementation which uses masks to achieve this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp6zRBT3vWca",
        "colab_type": "code",
        "outputId": "e1478b06-fe71-4e9d-8510-72c0507ec100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "k = 50\n",
        "\n",
        "model_weight = model.get_weights()\n",
        "pruned_layerwise_weights = prune_model(model_weights, k, prune_layer_units) \n",
        "flattened_input_shape = pruned_layerwise_weights[0].shape[0]\n",
        "\n",
        "# row_bool_mask removes rows (corresponding to connection with a specific\n",
        "# neuron from the previous layer for all the neurons in the current layer)\n",
        "# initialize to True because we don't prune inputs\n",
        "row_bool_mask = tf.fill([flattened_input_shape], True)\n",
        "\n",
        "minimized_hidden_sizes = []\n",
        "minimized_layerwise_weights = []\n",
        "for pruned_layer_weights in pruned_layerwise_weights:\n",
        "  \n",
        "  layer_weights = tf.identity(pruned_layer_weights)\n",
        "  # remove rows based on the neurons from the previous layers that have been\n",
        "  # pruned\n",
        "  layer_weights = tf.boolean_mask(layer_weights, row_bool_mask)\n",
        "  \n",
        "  # find which columns are all zeros and create a mask appropriately\n",
        "  intermediate_tensor = tf.reduce_sum(\n",
        "      tf.abs(layer_weights), axis=0)\n",
        "  zero_vector = tf.zeros(shape=(1,1), dtype=tf.float32)\n",
        "  col_bool_mask = tf.squeeze(tf.not_equal(intermediate_tensor, zero_vector))\n",
        "  \n",
        "  # transpose -> prune -> transpose of transpose to get back needed shape\n",
        "  layer_weights = tf.transpose(layer_weights)\n",
        "  layer_weights = tf.boolean_mask(layer_weights, col_bool_mask)\n",
        "  layer_weights = tf.transpose(layer_weights)\n",
        "  \n",
        "  minimized_hidden_sizes.append(layer_weights.shape[1].value)\n",
        "  minimized_layerwise_weights.append(layer_weights.numpy())\n",
        "  row_bool_mask = col_bool_mask\n",
        "  \n",
        "# remove output size\n",
        "minimized_hidden_sizes = minimized_hidden_sizes[:-1]\n",
        "\n",
        "mini_network = create_network(input_shape, minimized_hidden_sizes, output_size)\n",
        "mini_network.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.losses.softmax_cross_entropy,\n",
        "    metrics=['accuracy']\n",
        "    \n",
        ")\n",
        "mini_network.set_weights(minimized_layerwise_weights)\n",
        "mini_network.evaluate(test_data)\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 7ms/step - loss: 0.4037 - acc: 0.8796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.40370800431016124, 0.8796]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xrwMQ5W1rAx",
        "colab_type": "text"
      },
      "source": [
        "Same accuracy for 50% sparsity of units as the larger network containing the zero vectors!\n",
        "\n",
        "This will obviously speed up execution because it does smaller computations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpUvieg_18TY",
        "colab_type": "text"
      },
      "source": [
        "### Weight Level Sparsity\n",
        "\n",
        "We could use tf.sparse and convert our dense tensors with many many zeros to a sparse tensor to make it more efficient in terms of memory.\n",
        "\n",
        "I am not sure using sparse tensors would lead to an obvious speedup for the small network but for very sparse tensors which would have very large sizes I can completely see sparse version of the operations needed at inference time like matrix multiplication, maximum (for relu) lead to quicker execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlJb3HeqoP0l",
        "colab_type": "code",
        "outputId": "345704bd-c104-4682-a1eb-d869ee6ca6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "%%html\n",
        "<marquee style='height=100px; width: 100%; color: red;'><b>That's all folks!</b></marquee>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<marquee style='height=100px; width: 100%; color: red;'><b>That's all folks!</b></marquee>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}